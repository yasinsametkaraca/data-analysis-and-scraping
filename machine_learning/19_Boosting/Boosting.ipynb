{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geleneksel olarak ML uygulamaları şöyle çalışır:\n",
    "* Bir tane learner (algoritma) alırsınız, Logistic Regression, Decision Tree, Support Vector Machines...\n",
    "* Bu learner'a datayı beslerseniz\n",
    "* Bu learner'ı verdiğiniz data ile eğitirsiniz\n",
    "* Eğitilmiş bu learner ile tahmin yaparsınız (test data ile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fakat **ensemble metodlarında** durum bu şekilde işlemez.\n",
    "\n",
    "Ensemble (**bagging & boosting**) metodlarında, bir değil **bir kaç learner** alırsınız.\n",
    "\n",
    "Örneğin, Random Forest'larda olduğu gibi 1 adet değil, **n** adet Decision Tree alırsınız.\n",
    "\n",
    "Ve ML uygulamasının toplam performansı tüm bireysel learner'lardan (DT) daha fazla olur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging vs. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble Teknikleri:** Bir grup **weak learner** (basit algoritamalar, ör: DT) alarak onları **tek bir güçlü learner** haline getiren tekniklerdir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İki tür Ensemble Learning Metodu vardır:\n",
    "* Bagging (Random Forests)\n",
    "* Boosting (AdaBoost,XGBoost...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging** ile **Boosting** arasındaki fark şudur:\n",
    "* Bagging'de weak learner'lar paralel olarak eğitilir\n",
    "* Boosting'de weak learner'lar sıra ile (seri olarak) eğitilir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/bagging_vs_boosting.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$BAGGING$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/bagging.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$BOOSTING$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/boosting.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukarıdaki resimlerde gördüğünüz gibi, \n",
    "* Boosting'de modeller sırası ile işler ve her bir modelin farklı bir önem katsayısı vardır. (Sarı kutuların büyüklükleri modelin katsayısını gösterir)\n",
    "* Bagging'de ise, modeller paralel olarak işler ve tüm modeller aynı önem katsayısına sahiptirler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ek olarak:\n",
    "* Boosting'de veri noktaları da ağırlıklandırılmıştır (mor noktaların büyüklükleri katsayıları gösterir)\n",
    "  \n",
    "  Bir önceki algoritmanın yanlış sınıflandırdığı noktaların katsayısı büyültülür ki sıradaki algoritma o noktayı doğru sınıflandırmak için daha çok uğraşsın.\n",
    "  \n",
    "* Bagging'de tüm nonktlar aynı önemdedir ve veri seti içinden rasgele seçilirler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting nasıl bu kadar iyi çalışır?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genel olarak, `Ensemble Metodları` **hem Bias'ı hem de Variance'ı azaltırlar.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Metodları, tek bir algoritmaya (learner) dayanmayıp, n adet learner ile çalışırlar.\n",
    "\n",
    "Dolayısı ile **daha stabil ve güvenilir** olurlar. (Ör: Random Forests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting**'de N adet model bir sıra içinde train edilir. \n",
    "\n",
    "Bu sıranın özel bir amacı vardır.\n",
    "\n",
    "Her bir model (weak learner) bir önceki modelin hatalı sınıflandırdığı data noktalarını alır, onlara daha çok önem (katsayı) verir ve bu noktaları doğru sınıflandırmaya çalışır.\n",
    "\n",
    "Böylece her bir model (weak learner) bir önceki modelin hatalarını telafi eder. (Hatlarından öğrenir de diyebiliriz.)\n",
    "\n",
    "Bu şekilde model sayısı arttıkça toplam hata azalır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/boosting_learners.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting tek bir model değildir, bir çok modeli sırası ile çalıştıran jenerik bir algoritmalar dizisidir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Türleri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost (Adaptive Boosting) metodunda her bir weak learner, \n",
    "* yanlış classify edilmiş noktları alır ve onların ağırlıklarını artırır,\n",
    "* doğru classify edilmiş noktların ağırlıklarını düşürür\n",
    "* böylece kendisinden sonraki learner, yanlış tahmin edilmiş noktalara daha çok odaklanmış olur\n",
    "* bu şekilde yanlış noktalar düzeltilir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/adaboost_1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/adaboost_2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoost'un Hata Fonksiyonu:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her bir Weak Classifier C için:\n",
    "* X: n*d\n",
    "* Y: n*k\n",
    "* W: n*1\n",
    "\n",
    "olmak üzere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/adaboost_error.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ I(C(X_j) \\ne Y_j) $\n",
    "\n",
    "fonksiyonu, \n",
    "* eğer $C(X_j)$ (tahmin) $Y_j$ (gerçek y) eşit değilse **1** döner.\n",
    "* eğer eşitlerse **0** döner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yani amaç yanlış tahmin edilmiş noktaları tespit edip, onların ağırlıklarını artırmaktır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoost** taki 'Adaptive' kelimesi de buradan gelir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/adaboost_classifier.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting metodu soruna biraz daha farklı yaklaşır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data noklarının ağırlıklarını artırmaya çalışmak yerine, tahmin ile gerçek değer arasındaki farka odaklanır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amaç toplam Loss Fonksiyonu üzerinden giderek her bir iterasyondaki Weak Learner'ın toplam Loss'unu minimize etmektir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu çalışma şekli aslında bize Gradient Descent'i hatırlatır.\n",
    "\n",
    "Gradient Descent'te rasgele bir noktadan başlayıp toplam Loss'u azaltarak optimum noktaya varmaya çalışıyordu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İşte Gradient Boosting, Gradient Descent'e benzer ama tek farkla, artık optimize edilen tek bir noktanın Loss function'ı değil, **Model'in Loss Function'ıdır.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting'i modelleri optimize eden Gradient Descent olarak da düşünebiliriz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F$ modeli Gradient Boosting içindeki bir model olsun (weak learner):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F Modelini iyileştirmek için:** \n",
    "\n",
    "Tek bir modelin Loss fonksiyonu olan;\n",
    "\n",
    "$Loss(Y, F(X))$ \n",
    "\n",
    "değerini minimize etmemiz lazım."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting içindeki bütün modellerin kümüle Loss fonksiyonu ise:\n",
    "\n",
    "$L = func(F(X_1), F(X_2), ..., F(X_n), Y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bütün modellerin toplam Loss değeridir.\n",
    "\n",
    "İşte Gradient Boosting bu toplam Loss değerini optimize etmeye çalışır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/gradient_boosting.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://xgboost.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost (eXtreme Gradient Boosting)** bir dağıtık (distributed) Gradient Boosting metodudur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting'in sorunu modellerin sırası ile çalışmasıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu da zaman kaybı ve maliyet demektir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost modelleri paralel çalıştırarak hem daha hızlı hem de daha verimli çalışır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Endüstri'de kullanılan boosting algoritması çoğu yerde **XGBoost**'tur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle** problemlerinin büyük bir çoğunluğunda kazanan ML metodu **XGBoost**'tur."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
